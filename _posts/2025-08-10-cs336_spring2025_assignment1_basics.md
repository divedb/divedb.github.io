---
date: 2025-08-10
layout: post
title: cs336_spring2025_assignment1_basics
categories: cs336
tags: [cs336, AI] 
---



##### 2.4 `BPE`分词器训练

`BPE`分词器的训练过程主要分为三个步骤：

1.   词表初始化
     分词器的词表是一个从字节串`token`到整数`ID`的一一映射。由于我们要训练的是字节级`BPE`分词器，初始词表直接由所有可能的字节组成。因为字节值的取值范围是0~255，一共256种，因此初始词表的大小为256。

2.   预分词（`Pre-tokenization`）
     在获得词表后，理论上我们可以直接统计语料中相邻字节的出现频率，然后从最常见的字节对开始进行合并。但这种方法计算量很大——因为每次合并后，都需要对整个语料重新扫描一遍。此外，如果直接在字节层面跨整个语料合并，还可能出现一个问题：一些只在标点上不同的`token`（例如`dog!`与`dog.`）会被分配完全不同的`token ID`，尽管它们的语义几乎相同（只是标点不同）。

     为了解决这个问题，我们会先进行预分词。你可以把它理解为：先对语料进行一次**粗粒度的切分**，帮助我们更高效地统计相邻字符对的频率。例如，假设单词`"text"`在语料中出现了10次，那么我们只需在预分词的统计中，把`'t'`与`'e'`相邻的次数加上10，而不必逐字扫描语料中的每一次出现。由于我们训练的是字节级 BPE 模型，每个预分词单元（`pre-token`）都会被表示为一串`UTF-8`字节序列。

     在最早由`Sennrich`等人（2016）提出的`BPE`实现中，预分词只是简单地按空格分割（`s.split(" ")`）。相比之下，我们采用一种**基于正则表达式的预分词器**（来自`GPT-2`的实现，`Radford`等人，2019），可参考：[github.com/openai/tiktoken/pull/234/files](https://github.com/openai/tiktoken/pull/234/files)

```python
>>> PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
```

```python
>>> import regex as re
>>> re.findall(PAT, "some text that i'll pre-tokenize")
['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
```

实际使用时，建议使用`re.finditer`，这样在从预分词结果构建“预分词 → 频次”的映射时，就**不必把所有预分词结果一次性存储起来**。

<hr/>

1.   `(?:...)`里的`?`

-   `?:` 表示**非捕获分组**（`non-capturing group`），作用是把括号里的内容当作一个整体匹配，但不保存成捕获组。

-   例子：

    ```python
    (abc)   # 捕获组，匹配"abc"并保存到group(1)
    (?:abc) # 非捕获组，匹配"abc"但不保存
    ```

在`'(?:[sdmt]|ll|ve|re)`中，意思是：匹配`'s`、`'d`、`'m`、`'t`或`'ll`、`'ve`、`'re`这些缩写形式。

2.   ` ?\p{L}+`、` ?\p{N}+`、` ?[^\s\p{L}\p{N}]+` 中的`?`

-   这里的`?`是**量词**，表示**前一个字符（空格）可有可无**（匹配0或1次）。
-   例如` ?\p{L}+`表示：
    -   可选的一个空格，然后
    -   一个或多个字母（`\p{L}+`，`\p{L}`是`Unicode`属性，表示字母字符）。
-   类似地：
    -   ` ?\p{N}+` → 可选空格 + 一个或多个数字
    -   ` ?[^\s\p{L}\p{N}]+` → 可选空格 + 一个或多个既不是空白、字母、数字的字符（即符号、标点等）

3.   `\s+(?!\S)`中的`?`

-   这里的`?`出现在`(?!...)`中，表示**负向前瞻**（negative lookahead）。
-   `(?!\S)` 的意思是：
    -   当前位置**后面**不能是非空白字符（`\S`）。
    -   所以`\s+(?!\S)`匹配的是**结尾处的空白**（空格、换行等，且后面不是字母数字等可见字符）。

4.   单独的`\s+`

-   最后一个`\s+`就是匹配一个或多个空白字符。

<hr/>

3.   计算`BPE`合并

现在，我们已经将输入文本转换为**预分词**（`pre-tokens`），并将每个预分词表示为`UTF-8`字节序列，接下来就可以计算`BPE` 合并（也就是训练`BPE`分词器）。

从整体流程来看，`BPE`算法会**迭代**执行以下步骤：

1.  **统计**语料中所有相邻字节对的出现频率；
2.  找出出现频率最高的字节对（例如 `("A", "B")`）；
3.  将该字节对的所有出现位置**合并为一个新的token**（记作`"AB"`）；
4.  将这个新`token`添加到词表中。

最终，经过`BPE`训练后的词表大小 = **初始词表大小**（本例为256）+ **训练中执行的合并次数**。

为了提升训练效率，`BPE`合并时**不会**统计跨越预分词边界的字节对。

在选择需要合并的字节对时，如果有多个字节对的频率相同，我们会按**字典序**选择最大的那一个。例如，如果以下字节对的频率相同：

```python
("A", "B"), ("A", "C"), ("B", "ZZ"), ("BA", "A")
```

那么我们会选择合并`("BA", "A")`：

```python
>>> max([("A", "B"), ("A", "C"), ("B", "ZZ"), ("BA", "A")])
('BA', 'A')
```

4.   特殊`token`

在实际应用中，通常会有一些特定的字符串（例如`<|endoftext|>`）用来表示元数据（比如文档之间的边界）。在编码文本时，我们希望这些字符串始终作为**不可拆分的特殊`token`**保留，即**永远不会被分成多个`token`**。

例如，序列结束符`<|endoftext|>` 必须始终保持为一个`token`（对应一个固定的整数`ID`），这样语言模型才能在生成时正确识别何时停止。这些特殊`token`需要提前加入到词表中，并为它们分配对应的固定`token ID`。

`Sennrich`等人（2016）的算法给出了一种效率较低的`BPE`分词器训练实现（基本遵循了上面的步骤）。作为一个初步练习，可以尝试自己实现并测试这个函数，以检验你对`BPE`训练流程的理解。

**示例（`bpe_example`）：`BPE`训练示例**

下面是`Sennrich`等人（2016）给出的一个简化示例。假设我们的语料包含以下文本：

```html
low low low low low  
lower lower widest widest widest  
newest newest newest newest newest newest
```

并且词表中包含一个特殊`token <|endoftext|>`。

1.   **词表初始化**
     我们以特殊`token <|endoftext|>`以及所有 256 个字节作为初始词表。

2.   **预分词**
     为了简化说明、聚焦合并过程，本示例中假设预分词仅通过空格进行切分。在进行预分词并统计后，我们得到以下词频表：

```python
{low: 5, lower: 2, widest: 3, newest: 6}
```

我们可以用一个字典来表示词频，键是字节元组（`tuple[bytes]`），值是出现次数，比如`{(l, o, w): 5, …}`。需要注意的是，在`Python`中即使是单个字节，也是一个`bytes`对象；`Python`没有单独的“字节”类型，同理也没有表示单个字符的                 `char`类型。

3.   合并步骤
     我们首先统计所有相邻字节对在词中出现的频率之和，例如：

```python
{lo: 7, ow: 7, we: 8, er: 2, wi: 3, id: 3, de: 3, es: 9, st: 9, ne: 6, ew: 6}
```

其中`'es'`和`'st'`的频率相同，我们根据字典序选择较大的对，即`('st')`。然后对预分词进行合并，结果变成：

```python
{(l, o, w): 5, (l, o, w, e, r): 2, (w, i, d, e, st): 3, (n, e, w, e, st): 6}
```

第二轮合并时，出现频率最高的相邻对是`(e, st)`，出现次数为9，我们继续合并，得到：

```python
{(l, o, w): 5, (l, o, w, e, r): 2, (w, i, d, est): 3, (n, e, w, est): 6}
```

持续进行合并后，最终的合并序列为：

```python
['s t', 'e st', 'o w', 'l ow', 'w est', 'n e', 'ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r']
```

如果只进行前6次合并，则合并序列为：

```python
['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']
```

此时的词表元素为：

```python
[<|endoftext|>, [...256 个字节字符...], st, est, ow, low, west, ne]
```

使用这个词表和合并结果，单词`"newest"`会被分词为`[ne, west]`。

##### 2.5 `BPE`分词器训练实验

我们将基于`TinyStories`数据集训练一个字节级`BPE`分词器。关于数据集的获取和下载说明，请参考第1节。在开始训练之前，建议先浏览一下`TinyStories`数据集，了解数据的基本内容和结构。

**预分词的并行化**
预分词往往是训练中的主要瓶颈。你可以通过`Python`自带的 `multiprocessing` 库实现代码并行化，从而加速预分词过程。
具体来说，建议你在实现并行预分词时，将语料切分成多个块（`chunk`），并确保切分点总是出现在特殊`token`的开头位置。你可以直接使用下面链接中的示例代码来获得合理的切分边界，然后分发给多个进程并行处理： https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py

这种切分方法总是有效的，因为我们绝不会跨越文档边界进行合并。作业中，你也可以始终按照这种方式切分语料。无需担心遇到不包含`<|endoftext|>`的超大语料块的特殊情况。

**预分词前移除特殊`token`**
在使用正则表达式（如`re.finditer`）进行预分词之前，应当先将所有特殊`token`从语料（或并行时的语料块）中剥离。务必以特殊`token`为界切分语料，保证合并过程不会跨越这些分隔符。举例来说，若语料是`[Doc 1]<|endoftext|>[Doc 2]`，你应该先以`<|endoftext|>`为分隔符拆分，然后分别对`[Doc 1]`和`[Doc 2]`进行预分词。这可以通过`re.split`实现，使用 `"|".join(special_tokens)`作为分隔符（注意对特殊字符如`|`使用`re.escape`转义）。测试用例`test_train_bpe_special_tokens`会检验这一行为。

**合并步骤的优化**
上述示例中的`BPE`训练实现较为简单，但效率不高，因为每次合并都要遍历所有字节对以找出出现频率最高的对。实际上，每次合并后只有与被合并对相邻的字节对的计数发生变化。因此，可以通过建立索引缓存字节对的计数，并在合并后只对相关计数做增量更新，而不是每次都全量统计。采用这种缓存机制，可以显著提升训练速度。

不过需要注意的是，`BPE`的合并步骤在`Python`中**无法实现并行化**。

>低资源/缩减规模小贴士：性能分析
>
>建议你使用像**cProfile**或**scalene**这样的性能分析工具，找出实现中的性能瓶颈，并重点优化这些关键部分。



>低资源/缩减规模小贴士：“缩减规模”
>
>与其直接在完整的`TinyStories`数据集上训练分词器，我们推荐先在数据的一个小子集上训练，即“调试数据集”。
>比如，你可以用`TinyStories`的验证集（约2.2万篇文档），而不是完整的212万篇文档来训练分词器。
>
>这体现了一个通用的开发策略：**尽可能缩减规模以加快开发速度**，比如使用更小的数据集、更小的模型规模等。
>
>选择调试数据集的大小或超参数配置时需要谨慎权衡：
>
>-   数据集要足够大，能够反映出完整配置下的主要性能瓶颈，确保优化工作具有普适性；
>-   但又不能太大，否则训练时间过长，影响迭代效率。

**任务要求**

编写一个函数，给定输入文本文件的路径，训练一个（字节级）`BPE`分词器。你的`BPE`训练函数至少应支持以下输入参数：

-   `input_path: str` —— 训练数据文本文件的路径。
-   `vocab_size: int` —— 一个正整数，定义最终词表的最大规模（包括初始的字节词表、合并生成的新词条，以及任何特殊`token`）。
-   `special_tokens: list[str]` —— 需要加入词表的特殊字符串列表，这些特殊`token`不影响`BPE`训练过程。

你的`BPE`训练函数应返回训练结果：

-   `vocab: dict[int, bytes]` —— 分词器词表，是从整数`token ID`映射到对应字节序列的字典。
-   `merges: list[tuple[bytes, bytes]]` —— `BPE`合并操作序列。列表中每一项是一个字节对元组`<token1>, <token2>`，表示`<token1>`与`<token2>`被合并。合并顺序必须保持训练时的先后顺序。

为了测试你的`BPE`训练函数，请先实现测试适配器`[adapters.run_train_bpe]`，然后运行命令：

```bash
$ uv run pytest tests/test_train_bpe.py
```

确保你的实现能够通过所有测试。

**可选项**
如果你愿意（虽然可能投入较多时间），可以使用系统编程语言实现训练的关键部分，比如`C++`（可考虑`cppyy`）或`Rust`（使用`PyO3`）。如果采用此方案，请注意哪些操作需要复制数据，哪些可以直接从`Python`内存读取，并务必留下构建说明，或确保仅通过`pyproject.toml`就能构建。

此外，需要注意`GPT-2`使用的正则表达式在大多数正则引擎中支持不佳且执行缓慢。我们验证过，`Oniguruma`引擎既支持负向前瞻，又有较好性能；而`Python`的`regex`包性能更优。

-   使用`TinyStories`数据集训练一个字节` BPE`分词器，最大词表规模设为10,000。确保将`TinyStories`的特殊`token <|end`oftext|>`加入词表。将训练得到的词表和合并列表序列化保存到磁盘，便于后续查看。训练花费了多少小时和内存？词表中最长的`token`是什么？它是否合理？

    **资源要求**：训练时间不超过30分钟（不使用`GPU`），内存不超过`30GB`。

    **提示**：利用预分词时的多进程加速，并结合以下两点，训练时间应控制在2分钟以内：

    -   `<|endoftext|> token`用于划分数据文件中的文档边界。

    -   `<|endoftext|> token`在应用`BPE`合并前被特殊处理。

-   对代码进行性能分析。分词器训练过程中哪一步耗时最多？

接下来，我们将尝试在`OpenWebText`数据集上训练一个字节级`BPE`分词器。和之前一样，建议先浏览数据集，了解其内容。

-   在`OpenWebText`数据集上训练字节级`BPE`分词器，最大词表规模设为 32,000。将训练得到的词表和合并列表序列化保存到磁盘，便于后续查看。词表中最长的 token 是什么？它是否合理？
    -   资源要求：训练时间不超过12 小时（不使用`GPU`），内存不超过`100GB`。

-   对比分析你在`TinyStories`和`OpenWebText`上训练得到的分词器，有何异同？