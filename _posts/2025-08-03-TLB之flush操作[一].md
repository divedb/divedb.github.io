---
date: 2025-08-03
layout: post
title: TLB之flush操作[一]
categories: Linux
tags: [Linux, 缓存] 
---

`TLB`作为一种`cache`，也需要维护（和页表`PTE`的）一致性，区别在于普通`cache`对应的是属于物理硬件的内存，`CPU`可以维护`cache`和内存的一致性。而`TLB`对应的是`page table`（一种软件的数据结构），因此需要软件（操作系统）去维护`TLB`和`page table`的一致性。

在页表`PTE`的内容出现变化时，比如`page fault`时页面被换出，`munmap()`时映射被解除，就需要`invalidate`对应的`TLB entry`，有时这个操作也被称为`flush`（以下的讨论将统一称为`flush`）。

但这个`flush`和普通`cache`的`flush`是不一样的，它相当于是将某`TLB entry`清除，而不是刷到外部页表去同步，因为也不会有`TLB`里的数据比页表更新的情况，这是`TLB`和普通`cache`的又一区别。早期的处理器，比如`intel`的`80386`只支持`TLB`的全部`flush`，`80486`之后的`x86`处理器开始支持对`TLB`某个`entry`的单独刷新（`selective flushing`）。

发生`context switch`的时候通常也需要对`TLB`进行`flush`操作，`context switch`有两种，一种是某进程（设为`A`）通过`system call`（或其他方式）进入了`kernel mode`，内核处理完后再返回`user mode`，一种是进程切换（其实也是`user mode->kernel mode->user mode`）。

对于第一种情况，无论是`A`进程的页表对应的`TLB entries`，还是内核的页表对应的`TLB entries`，都是不需要被`flush`的。试想一下，如果进入`kernel mode`的时候`flush`了整个`TLB`，那`kernel`将面对一个空的`TLB`，需要过一段时间，让`kernel`的常用`PTE`进入`TLB`后，才能让`TLB`再次展现它的优势。

那如果保持`kernel`的`TLB entries`不变，只`flush`进程`A`的`TLB entries`呢？这样在返回`user mode`的时候，TLB虽然不是整个空的，但对进程`A`来说确是空的。这种用户空间和内核空间的切换在现实应用中是非常频繁的，如果采用`TLB flush`的策略将会严重性能。

对于第二种情况，由于`TLB`是采用虚拟地址的位域子集作为`tag`的，而不同的进程可能会有相同的虚拟地址，所有共用`TLB`的时候会发生冲突（傻傻分不清）。

![img](https://pic1.zhimg.com/v2-0a957b4b13cdeec6f3981c25462d7cb2_1440w.jpg)

但是在发生进程切换的时候，`kernel`的页表是不会有大的变化的，那`kernel`对应的`TLB entries`可以不被`flush`么？

在`linux`的内存模型中，用户空间和内核空间所用的虚拟地址是不会重叠的，因此理论上可以通过比对虚拟地址所在的内存范围（比如在32位系统中大于`3GB`的地址部分）来识别哪些`TLB entries`是属于`kernel`的。除了`full associative`的`TLB`，其他`n-way set`的`TLB`中的`tag`只是虚拟地址的一部分（详情请参考[这篇文章](https://zhuanlan.zhihu.com/p/65348145)），所以这种判断方法实现起来比较困难。

一个更简单的方法是在`TLB entry`中设一个标志位。现代处理器（`ARM`和`x86`）采用一个名为`G(Global)`的`bit`，来代表那些在系统运行中不常变化的映射关系，比如`kernel`和虚拟化里的`hypervisor`。以`x86`为例，当`CR3`寄存器中的内容被更新（因为`CR3`是指向进程页表的首地址的，这意味是发生了进程切换），默认会`flush`整个`TLB`。

如果在[CR4寄存器](https://zhida.zhihu.com/search?content_id=103011218&content_type=Article&match_order=1&q=CR4寄存器&zhida_source=entity)里置位了PGE（page global enable），则TLB里的G标志位就生效了，含有G的TLB entries就不会被flush了，成了钉子户了。当然，reset的时候，钉子户也是会被清掉的。

kernel的问题算是解决了，那有没有考虑过进程的感受呢。试想一下，在发生进程切换的时候，如果对非kernel的TLB全部flush，则意味着新换入的进程（设为B）开始执行的时候，TLB对于B进程来说是空的，B的执行性能会受到影响。如果B进程只执行了很短一段时间，就又切回了A进程，那整个性能表现就更差了。TLB总是冷的，你让进程怎么办。

linux里采用PID（Process ID）来区别和管理不同的进程，类似的，我们也可以在TLB中加上这样一个tag，如果虚拟地址的tag相同，就再比较这个标识进程的tag，这样就可以分清了，不会冲突了。在x86里这叫**[PCID](https://zhida.zhihu.com/search?content_id=103011218&content_type=Article&match_order=1&q=PCID&zhida_source=entity)** （Process Context Identifiers），在ARM里这叫**[ASID](https://zhida.zhihu.com/search?content_id=103011218&content_type=Article&match_order=1&q=ASID&zhida_source=entity)**（Address Space Identifiers）。其实，G标志位可以看做是内核的PCID/ASID，这样两者在概念上面就统一了。

![img](https://pic1.zhimg.com/v2-f7aa4b9e6dcf751a2220f1c4038f976e_1440w.jpg)

那TLB在缓存页表PTE的时候，怎么知道该PTE所属进程的PCID/ASID呢？以intel的x64为例，CR3寄存器了存储了当前进程对应的PCID（AMD64里是没有的）。

![img](https://pic1.zhimg.com/v2-1ac3253d77dc4282a73ed95442b08418_1440w.png)

置位CR4寄存器的PCIDE（PCID Enable）位后，PCID就生效了，进程切换的时候就不需要flush TLB了。但是CR3中的PCID只占12个bits，也就是说，它最多能表达4096个process。